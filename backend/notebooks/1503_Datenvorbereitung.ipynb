{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finaler Bericht - folgendes müssen wir noch gemeinsam prüfen:\n",
    "   1. Pfad struktur und aufruf der Funtkionen? - ist das jeweils logisch?\n",
    "   2. Bericht wird autoamtisch zum Experiment hinterlegt - dementpsrechend ist keine zeitstempel zur funtkion hinzufügen?\n",
    "   3. Auch beim Ursprungsdatensatz sollten wir diese Experimente ausführen und für die Preprocessing Pipeline berücksichtigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML-Bericht wurde erstellt: report.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from jinja2 import Template\n",
    "from scipy.stats import chi2_contingency, pearsonr\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "#Pfad ggfs. anpassen\n",
    "Alter_Pfad = \"data/df_income.csv\"\n",
    "Neuer_Pfad = \"data/df_income2.csv\"\n",
    "\n",
    "# Lade Datensätze\n",
    "df_orig = pd.read_csv(Alter_Pfad)\n",
    "df_new = pd.read_csv(Neuer_Pfad)\n",
    "\n",
    "# Erstelle einen Bericht\n",
    "def compare_datasets(df_orig, df_new):\n",
    "    report = {}\n",
    "    orig_columns = set(df_orig.columns)\n",
    "    new_columns = set(df_new.columns)\n",
    "    \n",
    "    report[\"fehlende_spalten\"] = list(orig_columns - new_columns)\n",
    "    report[\"neue_spalten\"] = list(new_columns - orig_columns)\n",
    "    \n",
    "    dtype_changes = {col: (str(df_orig[col].dtype), str(df_new[col].dtype)) \n",
    "                     for col in orig_columns.intersection(new_columns) if df_orig[col].dtype != df_new[col].dtype}\n",
    "    report[\"geaenderte_datentypen\"] = dtype_changes\n",
    "    \n",
    "    numeric_cols = df_orig.select_dtypes(include=[np.number]).columns.intersection(\n",
    "        df_new.select_dtypes(include=[np.number]).columns\n",
    "    )\n",
    "    range_changes = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        orig_min, orig_max = df_orig[col].min(), df_orig[col].max()\n",
    "        new_min, new_max = df_new[col].min(), df_new[col].max()\n",
    "        range_changes[col] = {\"original\": (orig_min, orig_max), \"neu\": (new_min, new_max)}\n",
    "    report[\"numerische_abweichungen\"] = range_changes\n",
    "    \n",
    "    categorical_cols = df_orig.select_dtypes(include=[\"object\"]).columns.intersection(\n",
    "        df_new.select_dtypes(include=[\"object\"]).columns\n",
    "    )\n",
    "    category_changes = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        orig_categories = set(df_orig[col].dropna().unique())\n",
    "        new_categories = set(df_new[col].dropna().unique())\n",
    "        category_changes[col] = {\"original\": list(orig_categories), \"neu\": list(new_categories)}\n",
    "    report[\"kategorische_abweichungen\"] = category_changes\n",
    "    \n",
    "    return report\n",
    "\n",
    "report = compare_datasets(df_orig, df_new)\n",
    "\n",
    "# Erstelle Plots Ordner\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Funktion Verteilungsanalyse der unabhängigen Variablen\n",
    "def generate_side_by_side_plots(df_orig, df_new, col, prefix):\n",
    "    if col == \"income\":  \n",
    "        return None  \n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6)) \n",
    "    plt.subplots_adjust(wspace=0.7)  \n",
    "\n",
    "    if df_orig[col].dtype == \"object\":\n",
    "        sns.countplot(x=col, data=df_orig, hue=\"income\", ax=axes[0])\n",
    "        sns.countplot(x=col, data=df_new, hue=\"income\", ax=axes[1])\n",
    "        axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "        axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    else:\n",
    "        sns.histplot(df_orig, x=col, hue=\"income\", bins=30, kde=False, ax=axes[0])\n",
    "        sns.histplot(df_new, x=col, hue=\"income\", bins=30, kde=False, ax=axes[1])\n",
    "    \n",
    "    axes[0].set_title(f\"{col} - Ursprungsdatensatz\")\n",
    "    axes[1].set_title(f\"{col} - Neuer Datensatz\")\n",
    "    \n",
    "    plot_path = f\"plots/{prefix}_{col}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    return plot_path\n",
    "\n",
    "# Verteilungen der Zielvariablen income \n",
    "def plot_income_distribution(df_orig, df_new):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    plt.subplots_adjust(wspace=0.7)\n",
    "\n",
    "    \n",
    "    sns.countplot(x=\"income\", data=df_orig, ax=axes[0], hue=\"income\", palette=\"Set1\")\n",
    "    sns.countplot(x=\"income\", data=df_new, ax=axes[1], hue=\"income\", palette=\"Set1\")\n",
    "\n",
    "    axes[0].set_title(\"Verteilung der Zielvariable income - Ursprungsdatensatz\")\n",
    "    axes[1].set_title(\"Verteilung der Zielvariable income - Neuer Datensatz\")\n",
    "    \n",
    "    plot_path = \"plots/income_comparison.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    return plot_path\n",
    "\n",
    "income_plot = plot_income_distribution(df_orig, df_new)\n",
    "comparison_plots = [generate_side_by_side_plots(df_orig, df_new, col, \"comparison\") \n",
    "                    for col in df_orig.columns.intersection(df_new.columns) if col != \"income\"]\n",
    "comparison_plots = [p for p in comparison_plots if p]  \n",
    "\n",
    "\n",
    "# Funktion zur visualisierung neuer Variablen \n",
    "def plot_new_variables(df_new, new_columns):\n",
    "    new_plots = []\n",
    "    for col in new_columns:\n",
    "        if col == \"income\":\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        if df_new[col].dtype == \"object\":\n",
    "            sns.countplot(x=col, data=df_new, hue=\"income\", order=df_new[col].value_counts().index)\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "        else:\n",
    "            sns.histplot(df_new, x=col, hue=\"income\", bins=30, kde=True)\n",
    "\n",
    "        plt.title(f\"Neue Variable: {col}\")\n",
    "        plot_path = f\"plots/new_variable_{col}.png\"\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        new_plots.append(plot_path)\n",
    "\n",
    "    return new_plots\n",
    "\n",
    "# Funktion zur visualisierung der gelöschten Variablen - welche Informationen fehlen fürs Modell\n",
    "def plot_deleted_variables(df_orig, deleted_columns):\n",
    "    deleted_plots = []\n",
    "    for col in deleted_columns:\n",
    "        if col == \"income\":\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        if df_orig[col].dtype == \"object\":\n",
    "            sns.countplot(x=col, data=df_orig, hue=\"income\", order=df_orig[col].value_counts().index)\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "        else:\n",
    "            sns.histplot(df_orig, x=col, hue=\"income\", bins=30, kde=True)\n",
    "\n",
    "        plt.title(f\"Gelöschte Variable: {col}\")\n",
    "        plot_path = f\"plots/deleted_variable_{col}.png\"\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        deleted_plots.append(plot_path)\n",
    "\n",
    "    return deleted_plots\n",
    "\n",
    "# Plots für neue und gelöschte Variablen\n",
    "new_variable_plots = plot_new_variables(df_new, report[\"neue_spalten\"])\n",
    "deleted_variable_plots = plot_deleted_variables(df_orig, report[\"fehlende_spalten\"])\n",
    "\n",
    "# Nachstehend umfangreiche Analyse des neuen Datensatzes - Auf folgendes wird der Datensatz überprüft: \n",
    "# Allgemeine Verteilung, Duplikate, hohe Korrelation, ungleich verteilte Variablen, fehlende Werte, Verteilung der Zielvariable\n",
    "\n",
    "\n",
    "def analyze_dataset(Neuer_Pfad, target_col=\"income\", numeric_threshold=0.7, categorical_p_value=0.01, imbalance_threshold=80):\n",
    "   \n",
    "    # Daten laden\n",
    "    df = pd.read_csv(Neuer_Pfad)\n",
    "    report_s = {}\n",
    "    \n",
    "    # Allgemeine Beschreibung des Datensatzes\n",
    "    num_rows, num_cols = df.shape\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    report_s[\"Allgemeine Datenbeschreibung\"] = (f\"Der Datensatz hat {num_rows} Zeilen und {num_cols} Spalten.\\n\"\n",
    "                                              f\"- {len(numeric_cols)} numerische Spalten\\n\"\n",
    "                                              f\"- {len(categorical_cols)} kategoriale Spalten\\n\")\n",
    "    \n",
    "    # Duplikate prüfen (kritisch ab 0.5%)\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    duplicate_ratio = duplicate_count / num_rows * 100\n",
    "    report_s[\"Duplikate\"] = f\"Es gibt {duplicate_count} Duplikate ({duplicate_ratio:.2f}%). - Da weniger als 0,5% der Daten Duplikate sind, gehen wir davon aus, dass es wahre Beobachtungen sind und keine Bereinigung notwendig ist.\" \\\n",
    "        if duplicate_ratio < 0.5 else f\"Achtung! Es gibt {duplicate_count} Duplikate ({duplicate_ratio:.2f}%). Eine Bereinigung könnte notwendig sein.\"\n",
    "    \n",
    "    # Korrelationen (Pearson für numerisch, Cramér's V für kategorial, Eta-Squared für gemischt)\n",
    "    cols = df.columns\n",
    "    corr_matrix = pd.DataFrame(index=cols, columns=cols, dtype=float)\n",
    "\n",
    "    def cramers_v(contingency_table):\n",
    "        chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "        n = contingency_table.sum().sum()\n",
    "        phi2 = chi2 / n\n",
    "        r, k = contingency_table.shape\n",
    "        return np.sqrt(phi2 / min(r-1, k-1))\n",
    "    \n",
    "    def eta_squared(num_col, cat_col):\n",
    "        groups = [num_col[cat_col == cat].dropna() for cat in np.unique(cat_col)]\n",
    "        ss_total = np.var(num_col, ddof=1) * (len(num_col) - 1)\n",
    "        ss_between = sum(len(group) * (np.mean(group) - np.mean(num_col))**2 for group in groups)\n",
    "        return ss_between / ss_total if ss_total > 0 else 0\n",
    "\n",
    "    for col1 in cols:\n",
    "        for col2 in cols:\n",
    "            if col1 == col2:\n",
    "                corr_matrix.loc[col1, col2] = 1.0\n",
    "            elif df[col1].dtype in [\"int64\", \"float64\"] and df[col2].dtype in [\"int64\", \"float64\"]:\n",
    "                corr_matrix.loc[col1, col2] = pearsonr(df[col1].dropna(), df[col2].dropna())[0]\n",
    "            elif df[col1].dtype == \"object\" and df[col2].dtype == \"object\":\n",
    "                contingency = pd.crosstab(df[col1], df[col2])\n",
    "                corr_matrix.loc[col1, col2] = cramers_v(contingency)\n",
    "            else:\n",
    "                cat_col, num_col = (df[col1], df[col2]) if df[col1].dtype == \"object\" else (df[col2], df[col1])\n",
    "                corr_matrix.loc[col1, col2] = eta_squared(num_col, cat_col)\n",
    "\n",
    "    # Top 3 Korrelationen zur Zielvariable\n",
    "    if target_col in corr_matrix.columns:\n",
    "        target_correlation = corr_matrix[target_col].drop(index=target_col, errors=\"ignore\").abs().sort_values(ascending=False)\n",
    "        report_s[\"Top 3 Variablen mit hoechster Korrelation zur Zielvariable\"] = target_correlation.head(3).to_string()\n",
    "        report_s[\"Top 3 Variablen mit geringster Korrelation zur Zielvariable\"] = target_correlation.tail(3).to_string()\n",
    "    \n",
    "    # Starke gegenseitige Korrelationen (> 0.7)\n",
    "    high_corr_pairs = [(row, col, corr_matrix.loc[row, col]) for row in corr_matrix.index for col in corr_matrix.columns\n",
    "                       if row != col and abs(corr_matrix.loc[row, col]) > numeric_threshold]\n",
    "    report_s[\"Variablen mit hoher gegenseitiger Korrelation (>0,7)\"] = \"\\n\".join(\n",
    "        [f\" - {var1} & {var2} (Korrelation: {corr:.2f}) - ggfs. nur eine Variable fuer das Model relevant\" for var1, var2, corr in high_corr_pairs])\n",
    "\n",
    "\n",
    "    # Ungleich verteilte Variablen**\n",
    "    skewed_vars = []\n",
    "    for col in df.columns:\n",
    "        most_common_ratio = df[col].value_counts(normalize=True, dropna=True).max() * 100  # Anteil der häufigsten Kategorie\n",
    "        if most_common_ratio > imbalance_threshold:\n",
    "            skewed_vars.append((col, most_common_ratio))\n",
    "\n",
    "    if skewed_vars:\n",
    "        report_s[\"Ungleich verteilte Variablen\"] = \"Folgende Variablen sind stark unausgewogen (>80% in einer Kategorie):\\n\"\n",
    "        for var, ratio in skewed_vars:\n",
    "            report_s[\"Ungleich verteilte Variablen\"] += f\" - {var}: {ratio:.2f}% der Werte entfallen auf eine einzige Kategorie\\n. Eine Transformation koennte notwendig sein.\"\n",
    "    else:\n",
    "        report_s[\"Ungleich verteilte Variablen\"] = \"Keine extrem ungleich verteilten Variablen gefunden.\"\n",
    "\n",
    "      \n",
    "    # Fehlende Werte (NaN und \"?\"-Zeichen prüfen)\n",
    "    missing_values = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        num_missing_nan = df[col].isna().sum()  # Anzahl NaN-Werte\n",
    "        num_missing_question = (df[col] == \"?\").sum()  # Anzahl \"?\"-Werte\n",
    "        total_missing = num_missing_nan + num_missing_question  # Gesamte fehlende Werte\n",
    "\n",
    "        if total_missing > 0:\n",
    "            missing_ratio = total_missing / len(df) * 100\n",
    "            missing_values[col] = (total_missing, missing_ratio, num_missing_nan, num_missing_question)\n",
    "\n",
    "    if missing_values:\n",
    "        report_s[\"Fehlende Werte\"] = \"Folgende Spalten enthalten fehlende Werte (NaN oder '?'):\\n\"\n",
    "        for var, (total, ratio, nan_count, question_count) in missing_values.items():\n",
    "            report_s[\"Fehlende Werte\"] += f\" - {var}: {total} fehlende Werte ({ratio:.2f}%)\\n\"\n",
    "            report_s[\"Fehlende Werte\"] += f\"   - NaN-Werte: {nan_count}, '?' Werte: {question_count}\\n\"\n",
    "            if ratio < 5:\n",
    "                report_s[\"Fehlende Werte\"] += \"   - Empfehlung: Modus oder Mittelwert verwenden\\n\"\n",
    "            else:\n",
    "                report_s[\"Fehlende Werte\"] += \"   - Empfehlung: Imputation oder Entfernen pruefen\\n\"\n",
    "    else:\n",
    "        report_s[\"Fehlende Werte\"] = \"Keine fehlenden Werte (NaN oder '?') gefunden.\"\n",
    "\n",
    "\n",
    "    # Zielvariable überprüfen \n",
    "    target_col = \"income\"  # Zielvariable\n",
    "    if target_col in df.columns:\n",
    "        target_distribution = df[target_col].value_counts(normalize=True) * 100\n",
    "        imbalance_threshold = 70\n",
    "        majority_class = target_distribution.idxmax()\n",
    "        majority_ratio = target_distribution.max()\n",
    "\n",
    "        if majority_ratio > imbalance_threshold:\n",
    "            report_s[\"Zielvariable-Verteilung\"] = (f\"Achtung! Die Zielvariable '{target_col}' ist unausgeglichen.\\n\"\n",
    "                                                 f\"- Mehrheit der Daten ({majority_ratio:.2f}%) sind in der Klasse '{majority_class}'.\\n\"\n",
    "                                                 \"- Dies kann das Modell stark beeinflussen. In Betracht ziehen: Sampling-Methoden.\")\n",
    "        else:\n",
    "            report_s[\"Zielvariable-Verteilung\"] = f\"Die Zielvariable '{target_col}' ist relativ ausgewogen verteilt.\"\n",
    "    else:\n",
    "        report_s[\"Zielvariable-Verteilung\"] = \"Zielvariable 'income' nicht im Datensatz gefunden.\"\n",
    "\n",
    "\n",
    "    return report_s\n",
    "\n",
    "#Korrelationsmatrix erstellen und Bild abspeichern\n",
    "\n",
    "def compute_correlation_matrix(df):\n",
    "    cols = df.columns\n",
    "    corr_matrix = pd.DataFrame(index=cols, columns=cols, dtype=float)\n",
    "\n",
    "    def cramers_v(contingency_table):\n",
    "        chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "        n = contingency_table.sum().sum()\n",
    "        phi2 = chi2 / n\n",
    "        r, k = contingency_table.shape\n",
    "        return np.sqrt(phi2 / min(r-1, k-1)) if min(r-1, k-1) > 0 else 0\n",
    "\n",
    "    def eta_squared(num_col, cat_col):\n",
    "        groups = [num_col[cat_col == cat].dropna() for cat in np.unique(cat_col)]\n",
    "        ss_total = np.var(num_col, ddof=1) * (len(num_col) - 1)\n",
    "        ss_between = sum(len(group) * (np.mean(group) - np.mean(num_col))**2 for group in groups)\n",
    "        return ss_between / ss_total if ss_total > 0 else 0\n",
    "\n",
    "    for i, col1 in enumerate(cols):\n",
    "        for j, col2 in enumerate(cols[i:], i):  # Berechnet nur die obere Hälfte der Matrix\n",
    "            if col1 == col2:\n",
    "                corr_matrix.loc[col1, col2] = 1.0\n",
    "            elif df[col1].dtype in [\"int64\", \"float64\"] and df[col2].dtype in [\"int64\", \"float64\"]:\n",
    "                corr_matrix.loc[col1, col2] = corr_matrix.loc[col2, col1] = pearsonr(df[col1].dropna(), df[col2].dropna())[0]\n",
    "            elif df[col1].dtype == \"object\" and df[col2].dtype == \"object\":\n",
    "                contingency = pd.crosstab(df[col1], df[col2])\n",
    "                value = cramers_v(contingency)\n",
    "                corr_matrix.loc[col1, col2] = corr_matrix.loc[col2, col1] = value\n",
    "            else:\n",
    "                cat_col, num_col = (df[col1], df[col2]) if df[col1].dtype == \"object\" else (df[col2], df[col1])\n",
    "                value = eta_squared(num_col, cat_col)\n",
    "                corr_matrix.loc[col1, col2] = corr_matrix.loc[col2, col1] = value\n",
    "\n",
    "    return corr_matrix\n",
    "    \n",
    "\n",
    "\n",
    "correlation_matrix = compute_correlation_matrix(df_new)\n",
    "\n",
    "# Funktion ausführen und Ergebnisse anzeigen -  ggfs. Dateipfad anpassen!!\n",
    "final_analysis_report = analyze_dataset(Neuer_Pfad)\n",
    "\n",
    "\n",
    "# Bericht als DataFrame formatieren\n",
    "report_df = pd.DataFrame(list(final_analysis_report.items()), columns=[\"Kategorie\", \"Ergebnisse\"])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Korrelationsmatrix des neuen Datensatzes\")\n",
    "correlation_plot_path = \"plots/correlation_matrix.png\"\n",
    "plt.savefig(correlation_plot_path)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# HTML Bericht\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Automatisierte Untersuchung des neuen Datensatzes</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; margin: 40px; }\n",
    "        h1, h2 { color: #333; }\n",
    "        table { width: 100%; border-collapse: collapse; margin-bottom: 40px; }\n",
    "        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
    "        th { background-color: #f4f4f4; }\n",
    "        img { margin: 10px; max-width: 1200px; display: block; }\n",
    "        .image-container { display: flex; justify-content: center; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Automatisierte Untersuchung des neuen Datensatzes</h1>\n",
    "    <h2>Erstellt am {{ date }}</h2>\n",
    "        \n",
    "    <h2>Fehlende und neue Spalten</h2>\n",
    "    <table>\n",
    "        <tr><th>Fehlende Spalten</th><th>Neue Spalten</th></tr>\n",
    "        <tr>\n",
    "            <td>{{ report.fehlende_spalten }}</td>\n",
    "            <td>{{ report.neue_spalten }}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    \n",
    "    <h2>Numerische und Kategorische Abweichungen</h2>\n",
    "    <h3>Numerische Variablen</h3>\n",
    "    <table>\n",
    "        <tr><th>Spalte</th><th>Vorher (Min, Max)</th><th>Jetzt (Min, Max)</th></tr>\n",
    "        {% for col, changes in report.numerische_abweichungen.items() %}\n",
    "        <tr>\n",
    "            <td>{{ col }}</td><td>{{ changes.original }}</td><td>{{ changes.neu }}</td>\n",
    "        </tr>\n",
    "        {% endfor %}\n",
    "    </table>\n",
    "    \n",
    "    <h3>Kategorische Variablen</h3>\n",
    "    <table>\n",
    "        <tr><th>Spalte</th><th>Vorher Kategorien</th><th>Jetzt Kategorien</th></tr>\n",
    "        {% for col, changes in report.kategorische_abweichungen.items() %}\n",
    "        <tr>\n",
    "            <td>{{ col }}</td><td>{{ changes.original }}</td><td>{{ changes.neu }}</td>\n",
    "        </tr>\n",
    "        {% endfor %}\n",
    "    </table>\n",
    "    \n",
    "    <h2>Verteilung der Zielvariablen Income</h2>\n",
    "    <img src=\"{{ income_plot }}\">\n",
    "    \n",
    "    <h2>Verteilungsanalyse der unabhaengigen Variablen</h2>\n",
    "    {% for plot in comparison_plots %}\n",
    "        <div class=\"image-container\">\n",
    "            <img src=\"{{ plot }}\">\n",
    "        </div>\n",
    "    {% endfor %}\n",
    "    \n",
    "     <h2>Darstellung neuer Variablen</h2>\n",
    "    {% for plot in new_variable_plots %}\n",
    "        <div class=\"image-container\">\n",
    "            <img src=\"{{ plot }}\">\n",
    "        </div>\n",
    "    {% endfor %}\n",
    "\n",
    "    <h2>Darstellung fehlender Variablen</h2>\n",
    "    {% for plot in deleted_variable_plots %}\n",
    "        <div class=\"image-container\">\n",
    "            <img src=\"{{ plot }}\">\n",
    "        </div>\n",
    "    {% endfor %}\n",
    "     <h2>Eigenschaften und Hinweise zum neuen Datensatz</h2>\n",
    "        <table>\n",
    "        <tr>\n",
    "            <th>Kategorie</th>\n",
    "            <th>Ergebnisse</th>\n",
    "        </tr>\n",
    "        {% for row in report_s %}\n",
    "        <tr>\n",
    "            <td>{{ row.Kategorie }}</td>\n",
    "            <td>{{ row.Ergebnisse }}</td>\n",
    "        </tr>\n",
    "        {% endfor %}\n",
    "    </table>\n",
    "    <h2>Korrelationsmatrix des neuen Datensatzes</h2>\n",
    "        <img src=\"{{ correlation_plot }}\">\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "template = Template(html_template)\n",
    "html_content = template.render(date=datetime.datetime.now(), income_plot=income_plot, comparison_plots=comparison_plots, new_variable_plots=new_variable_plots, deleted_variable_plots=deleted_variable_plots, correlation_plot=correlation_plot_path, report_s=report_df.to_dict(orient=\"records\"), report=report)\n",
    "\n",
    "with open(\"report.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"HTML-Bericht wurde erstellt: report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocessing Pipeline für den Datensatz income auf Basis der EDA\n",
    "   1. Noch keinen Ausgleich der Zielvariablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def preprocess_data(file_path, save_path=\"data/processed_data.csv\"):\n",
    "    \"\"\"\n",
    "    Führt das Preprocessing für den Datensatz durch, basierend auf der explorativen Datenanalyse (EDA).\n",
    "    Parameter:\n",
    "    - `file_path` (str): Eingabedatei (CSV)\n",
    "    - `save_path` (str): Speicherpfad für verarbeitete Daten\n",
    "\n",
    "    Rückgabe:\n",
    "    - `processed_df`: Fertig vorverarbeitete Daten\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Laden der Daten\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 2. Ersetzung von \"?\" in `workclass` und `occupation` mit \"Unknown\" (weil >5% fehlend und die Manipulation ansonsten zu groß wäre)\n",
    "    df[\"workclass\"] = df[\"workclass\"].replace(\"?\", \"Unknown\")\n",
    "    df[\"occupation\"] = df[\"occupation\"].replace(\"?\", \"Unknown\")\n",
    "\n",
    "    # 3. Fehlende Werte in `native-country` mit Modus auffüllen (weil <5% fehlend)\n",
    "    df[\"native-country\"] = df[\"native-country\"].replace(\"?\", np.nan)\n",
    "    df[\"native-country\"].fillna(df[\"native-country\"].mode()[0], inplace=True)\n",
    "\n",
    "    #  4. Kategorisierung von `native-country` und `native-country` \n",
    "    df[\"native-country\"] = df[\"native-country\"].apply(lambda x: x if x == \"United-States\" else \"Other Countries\")\n",
    "    df[\"race\"] = df[\"native-country\"].apply(lambda x: x if x == \"White\" else \"Other\")\n",
    "\n",
    "    # 5. Entfernen redundanter Variablen\n",
    "    df.drop(columns=[\"education\"], inplace=True)  \n",
    "\n",
    "    #6. Kategorisierung von `capital-gain` und `capital-loss`\n",
    "    def categorize_gain_loss(value):\n",
    "        if value == 0:\n",
    "            return \"No Gain/Loss\"\n",
    "        elif value <= 5000:\n",
    "            return \"Low Gain/Loss\"\n",
    "        else:\n",
    "            return \"High Gain/Loss\"\n",
    "    \n",
    "    df[\"capital-gain-bin\"] = df[\"capital-gain\"].apply(categorize_gain_loss)\n",
    "    df[\"capital-loss-bin\"] = df[\"capital-loss\"].apply(categorize_gain_loss)\n",
    "    \n",
    "    \n",
    "    #7. Kodierung kategorialer Variablen\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    categorical_cols.remove(\"income\")  \n",
    "\n",
    "    encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "    encoded_data = encoder.fit_transform(df[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "    #8. Standardisierung numerischer Variablen\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    #9. Zielvariable kodieren\n",
    "    df[\"income\"] = df[\"income\"].map({\"<=50K\": 0, \">50K\": 1})  \n",
    "\n",
    "    # Speicherung der verarbeiteten Daten (Train/Test wird später gemacht!)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"✅ Preprocessing abgeschlossen! Gespeicherte Datei: {save_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Funktion ausführen - hierfür Pfad überprüfen! \n",
    "processed_df = preprocess_data(\"data/df_income.csv\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
